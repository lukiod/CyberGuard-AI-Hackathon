# -*- coding: utf-8 -*-
"""Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sogsqbuOA2i1qHZ_Hlhiw9UMRn7ZRhDI

# <font color ='red'>Note:<br>1. Change the file path of crime_report.csv <br>2. Some of the code are meant for cpu and some for gpu in case u fill its quite slow switch over to gpu <br>3. Use tutorial-env on VM and trch on GPU <br>4. if you are on vm it will take time  and use tutorial-env as a env which can be opened using a kernel on vs code and has python 3.12.8 and if u are trying to run this on gpu<br>5. i am listing pip freeze of both tutorial-env and trch tutorial-env for cpu and trch for gpu </font>
"""

!pip install transformers torch spacy scikit-learn numpy pandas tqdm matplotlib seaborn emoji
!python -m spacy download en_core_web_sm
!pip install openpyxl
!pip install pandas numpy matplotlib scikit-learn nltk wordcloud

"""### pip freeze of tutorial-env"""

absl-py==2.1.0
annotated-types==0.7.0
asttokens==3.0.0
blis==1.2.0
catalogue==2.0.10
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
cloudpathlib==0.20.0
colorama==0.4.6
comm==0.2.2
confection==0.1.5
contourpy==1.3.1
cycler==0.12.1
cymem==2.0.11
debugpy==1.8.12
decorator==5.1.1
emoji==2.14.1
et_xmlfile==2.0.0
executing==2.2.0
filelock==3.17.0
fonttools==4.56.0
fsspec==2025.2.0
grpcio==1.70.0
hdbscan==0.8.40
huggingface-hub==0.28.1
idna==3.10
ipykernel==6.29.5
ipython==8.32.0
jedi==0.19.2
Jinja2==3.1.5
joblib==1.4.2
jupyter_client==8.6.3
jupyter_core==5.7.2
kiwisolver==1.4.8
langcodes==3.5.0
language_data==1.3.0
llvmlite==0.44.0
marisa-trie==1.2.1
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.10.0
matplotlib-inline==0.1.7
mdurl==0.1.2
mpmath==1.3.0
murmurhash==1.0.12
narwhals==1.27.1
nest-asyncio==1.6.0
networkx==3.4.2
nltk==3.9.1
numba==0.61.0
numpy==2.1.3
openpyxl==3.1.5
packaging==24.2
pandas==2.2.3
parso==0.8.4
pillow==11.1.0
platformdirs==4.3.6
plotly==6.0.0
preshed==3.0.9
prompt_toolkit==3.0.50
protobuf==5.29.3
psutil==7.0.0
pure_eval==0.2.3
pydantic==2.10.6
pydantic_core==2.27.2
Pygments==2.19.1
pynndescent==0.5.13
pyparsing==3.2.1
python-dateutil==2.9.0.post0
pytz==2025.1
pywin32==308
PyYAML==6.0.2
pyzmq==26.2.1
regex==2024.11.6
requests==2.32.3
rich==13.9.4
safetensors==0.5.2
scikit-learn==1.6.1
scipy==1.15.2
seaborn==0.13.2
setuptools==75.8.0
shellingham==1.5.4
six==1.17.0
smart-open==7.1.0
spacy==3.8.4
spacy-legacy==3.0.12
spacy-loggers==1.0.5
srsly==2.5.1
stack-data==0.6.3
sympy==1.13.1
tensorboard==2.19.0
tensorboard-data-server==0.7.2
thinc==8.3.4
threadpoolctl==3.5.0
tokenizers==0.21.0
torch==2.6.0
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
typer==0.15.1
typing_extensions==4.12.2
tzdata==2025.1
umap-learn==0.5.7
urllib3==2.3.0
wasabi==1.1.3
wcwidth==0.2.13
weasel==0.4.1
Werkzeug==3.1.3
wordcloud==1.9.4
wrapt==1.17.2

"""## pip  freeze of trch env"""

absl-py==2.1.0
annotated-types==0.7.0
asttokens==3.0.0
astunparse==1.6.3
blinker==1.9.0
blis==1.2.0
catalogue==2.0.10
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
cloudpathlib==0.20.0
cmudict==1.0.32
comm==0.2.2
confection==0.1.5
contourpy==1.3.1
cupy-cuda12x==13.3.0
cycler==0.12.1
cymem==2.0.11
debugpy==1.8.12
decorator==5.1.1
en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl
exceptiongroup==1.2.2
executing==2.2.0
fastrlock==0.8.3
filelock==3.17.0
Flask==3.1.0
flatbuffers==25.2.10
fonttools==4.56.0
fsspec==2025.2.0
gast==0.6.0
google-pasta==0.2.0
grpcio==1.70.0
h5py==3.12.1
hdbscan==0.8.40
huggingface-hub==0.28.1
idna==3.10
importlib_metadata==8.6.1
importlib_resources==6.5.2
ipykernel==6.29.5
ipython==8.32.0
itsdangerous==2.2.0
jedi==0.19.2
Jinja2==3.1.5
joblib==1.4.2
jupyter_client==8.6.3
jupyter_core==5.7.2
keras==3.8.0
kiwisolver==1.4.8
langcodes==3.5.0
langdetect==1.0.9
language_data==1.3.0
libclang==18.1.1
llvmlite==0.44.0
marisa-trie==1.2.1
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib==3.10.0
matplotlib-inline==0.1.7
mdurl==0.1.2
ml-dtypes==0.4.1
mpmath==1.3.0
murmurhash==1.0.12
namex==0.0.8
nest-asyncio==1.6.0
networkx==3.4.2
nltk==3.9.1
numba==0.61.0
numpy==2.0.2
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-cusparselt-cu12==0.6.2
nvidia-nccl-cu12==2.21.5
nvidia-nvjitlink-cu12==12.4.127
nvidia-nvtx-cu12==12.1.105
opt_einsum==3.4.0
optree==0.14.0
packaging==24.2
pandas==2.2.3
parso==0.8.4
pexpect==4.9.0
pillow==11.1.0
platformdirs==4.3.6
preshed==3.0.9
prompt_toolkit==3.0.50
protobuf==5.29.3
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pydantic==2.10.6
pydantic_core==2.27.2
Pygments==2.19.1
pynndescent==0.5.13
pyparsing==3.2.1
pyphen==0.17.2
python-dateutil==2.9.0.post0
pytz==2025.1
PyYAML==6.0.2
pyzmq==26.2.1
regex==2024.11.6
requests==2.32.3
rich==13.9.4
safetensors==0.5.2
scikit-learn==1.6.1
scipy==1.15.2
seaborn==0.13.2
sentence-transformers==3.4.1
shellingham==1.5.4
six==1.17.0
smart-open==7.1.0
spacy==3.8.4
spacy-legacy==3.0.12
spacy-loggers==1.0.5
srsly==2.5.1
stack-data==0.6.3
sympy==1.13.1
tensorboard==2.18.0
tensorboard-data-server==0.7.2
tensorflow==2.18.0
tensorflow-io-gcs-filesystem==0.37.1
termcolor==2.5.0
textblob==0.19.0
textstat==0.7.5
thinc==8.3.4
threadpoolctl==3.5.0
tokenizers==0.21.0
torch==2.5.1+cu121
torchaudio==2.5.1+cu121
torchvision==0.20.1+cu121
tornado==6.4.2
tqdm==4.67.1
traitlets==5.14.3
transformers==4.49.0
triton==3.1.0
typer==0.15.1
typing_extensions==4.12.2
tzdata==2025.1
umap-learn==0.5.7
urllib3==2.3.0
wasabi==1.1.3
wcwidth==0.2.13
weasel==0.4.1
Werkzeug==3.1.3
wrapt==1.17.2
zipp==3.21.0

"""# Testing Word CLoud on a single file"""

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re  # for regular expressions
import csv  # Import the csv module

# Download necessary NLTK data (only needs to be done once)
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')  # Download punkt tokenizer if missing
    nltk.download('wordnet')  # Download wordnet if missing

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find("corpora/wordnet")
except LookupError:
    nltk.download('wordnet')


def generate_word_cloud_and_frequency(excel_file_path, text_column, output_image_path="wordcloud.png", top_n=50):
    """
    Generates a word cloud and lists the top N word frequencies from an Excel file.

    Args:
        excel_file_path (str): The path to the Excel file.
        text_column (str): The name of the column containing the text data.
        output_image_path (str, optional): The path to save the word cloud image.
            Defaults to "wordcloud.png".
        top_n (int, optional): The number of top words to list. Defaults to 50.

    Returns:
        tuple: A tuple containing:
            - pandas.Series: A Pandas Series containing the word frequencies, sorted in
                             descending order.  Returns None if there are errors.
            - list: A list of the top N words based on frequency. Returns None if
                    there are errors.
    """

    try:
        # 1. Load the Excel file into a Pandas DataFrame
        df = pd.read_excel(excel_file_path)  # Start with basic reading of excel

        # 2. Extract the text data from the specified column
        text_data = df[text_column].astype(str).str.cat(sep=' ')  # Handle potential NaN values and concatenate into a single string

        # 3. Text Cleaning
        text_data = text_data.lower()  # Convert to lowercase

        # Remove URLs
        text_data = re.sub(r'http\S+|www\S+|https\S+', '', text_data, flags=re.MULTILINE)

        #Remove emails
        text_data = re.sub(r'\S*@\S*\s?', '', text_data, flags=re.MULTILINE)

        #Remove punctuations and special characters
        text_data = re.sub(r'[^a-zA-Z\s]', '', text_data, flags=re.MULTILINE) #Keep letters and spaces
        text_data = re.sub(r'\s+', ' ', text_data).strip() #Remove extra spaces


        stop_words = set(stopwords.words('english')) #Get stop words
        word_tokens = word_tokenize(text_data)
        filtered_text = [word for word in word_tokens if word not in stop_words] #Remove stop words
        cleaned_text = ' '.join(filtered_text)

        # 4. Calculate Word Frequencies
        word_counts = pd.Series(cleaned_text.split()).value_counts() # Count the words

        # 5. Generate Word Cloud
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(cleaned_text)

        # Display the word cloud
        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.savefig(output_image_path)  # Save the word cloud image
        plt.show()  # Display the word cloud
        print(f"Word cloud saved to {output_image_path}")


        # 6. List Word Frequencies
        print(f"\nWord Frequencies (Top {top_n}):")  # Display top N for brevity
        print(word_counts.head(top_n)) #Prints top N word counts

        # Create the list of top N words:
        top_word_list = list(word_counts.head(top_n).index)  # Get the index (words) of the top N items

        return word_counts, top_word_list # Return both the frequency counts and the word list

    except FileNotFoundError:
        print(f"Error: File not found at {excel_file_path}")
        return None, None
    except KeyError:
        print(f"Error: Column '{text_column}' not found in the Excel file.")
        return None, None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None, None


# --------------------- Main Execution ---------------------
if __name__ == "__main__":
    excel_file = r"C:\Users\I4C\Desktop\CyberGuardAI\db\Hackathon\Advance Fee Fraud.xlsx"  # Replace with the actual path to your CSV file if it's not in the same directory
    column_name = "Crime Aditional Information"
    output_image = "crimeinfo_wordcloud1.png"
    top_n_words = 50  # Specify the number of top words you want

    word_frequencies, top_word_list = generate_word_cloud_and_frequency(excel_file, column_name, output_image, top_n_words)

    if word_frequencies is not None and top_word_list is not None:
        print("Analysis complete.  Check the console for results and", output_image, "for the word cloud.")
        print(f"\nList of top {top_n_words} words:")
        print(top_word_list) # Print the list of top words
    else:
        print("An error occurred during analysis.")

"""# Approch 1

## Getting word cloud and top 50 words of a file with frequency in a vector
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re  # for regular expressions
import csv  # Import the csv module

# Download necessary NLTK data (only needs to be done once)
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')  # Download punkt tokenizer if missing
    nltk.download('wordnet')  # Download wordnet if missing

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find("corpora/wordnet")
except LookupError:
    nltk.download('wordnet')


def generate_word_cloud_and_frequency(file_path, possible_text_columns, output_image_path="wordcloud.png", top_n=50):
    """
    Generates a word cloud and lists the top N word frequencies from a CSV or Excel file.
    Handles variations in text column names.

    Args:
        file_path (str): The path to the CSV or Excel file.
        possible_text_columns (list): A list of possible column names for the text data.
        output_image_path (str, optional): The path to save the word cloud image.
            Defaults to "wordcloud.png".
        top_n (int, optional): The number of top words to list. Defaults to 50.

    Returns:
        tuple: A tuple containing:
            - pandas.Series: A Pandas Series containing the word frequencies, sorted in
                             descending order.  Returns None if there are errors.
            - list: A list of the top N words based on frequency. Returns None if
                    there are errors.
    """

    try:
        # 1. Load the file into a Pandas DataFrame
        if file_path.endswith(".xlsx"):
            df = pd.read_excel(file_path)
        elif file_path.endswith(".csv"):
            df = pd.read_csv(file_path, encoding='utf-8')  # Try UTF-8 encoding
        else:
            print(f"Error: Unsupported file format: {file_path}. Only .csv and .xlsx files are supported.")
            return None, None

        # 2. Find the correct text column
        text_column = None
        for col in possible_text_columns:
            if col in df.columns:
                text_column = col
                break  # Stop searching once a match is found
            else:
                # Try case-insensitive matching:
                for df_col in df.columns:
                    if col.lower() == df_col.lower():  #Case insensitive comparison
                        text_column = df_col #Use actual name in dataframe
                        break
                if text_column: #break from outer loop too
                    break

        if not text_column:
            print(f"Error: No matching text column found in {file_path}.  Tried: {possible_text_columns}")
            return None, None

        # 3. Extract the text data from the specified column
        text_data = df[text_column].astype(str).str.cat(sep=' ')  # Handle potential NaN values and concatenate into a single string

        # 4. Text Cleaning
        text_data = text_data.lower()  # Convert to lowercase

        # Remove URLs
        text_data = re.sub(r'http\S+|www\S+|https\S+', '', text_data, flags=re.MULTILINE)

        #Remove emails
        text_data = re.sub(r'\S*@\S*\s?', '', text_data, flags=re.MULTILINE)

        #Remove punctuations and special characters
        text_data = re.sub(r'[^a-zA-Z\s]', '', text_data, flags=re.MULTILINE) #Keep letters and spaces
        text_data = re.sub(r'\s+', ' ', text_data).strip() #Remove extra spaces


        stop_words = set(stopwords.words('english')) #Get stop words
        word_tokens = word_tokenize(text_data)
        filtered_text = [word for word in word_tokens if word not in stop_words] #Remove stop words
        cleaned_text = ' '.join(filtered_text)

        # 5. Calculate Word Frequencies
        word_counts = pd.Series(cleaned_text.split()).value_counts() # Count the words

        # 6. Generate Word Cloud
        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(cleaned_text)

        # Display the word cloud
        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.savefig(output_image_path)  # Save the word cloud image
        plt.close() # Close the plot to free memory
        print(f"Word cloud saved to {output_image_path}")


        # 7. List Word Frequencies
        print(f"\nWord Frequencies (Top {top_n}):")  # Display top N for brevity
        print(word_counts.head(top_n)) #Prints top N word counts

        # Create the list of top N words:
        top_word_list = list(word_counts.head(top_n).index)  # Get the index (words) of the top N items

        return word_counts, top_word_list # Return both the frequency counts and the word list

    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return None, None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None, None



# --------------------- Main Execution ---------------------
if __name__ == "__main__":
    directory = r"C:\Users\I4C\Desktop\CyberGuardAI\db\Hackathon"  # Replace with the directory containing your files
    possible_text_columns = [
        "Crime Aditional Information",
        "Crime Additional Information",
        "crimeaditionalinfo",  # Correct spelling (just in case)
        "crimeinfo",
        "crimeInfo",
        "Crime Info",
        "Description",
        "Text",
        "crimeaditionalinfo" # Add as many variations as you can think of
    ]
    output_dir = "wordclouds"  # Directory to save the word cloud images
    top_n_words = 50

    # Create the output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Iterate through all files in the directory
    for filename in os.listdir(directory):
        if filename.endswith(".xlsx") or filename.endswith(".csv"):
            file_path = os.path.join(directory, filename)
            output_image_path = os.path.join(output_dir, f"{os.path.splitext(filename)[0]}_wordcloud.png")  # Create a unique name

            print(f"\nProcessing file: {filename}")
            word_frequencies, top_word_list = generate_word_cloud_and_frequency(
                file_path, possible_text_columns, output_image_path, top_n_words
            )

            if word_frequencies is not None and top_word_list is not None:
                print(f"Analysis complete for {filename}. Check console for results and {output_image_path} for the word cloud.")
                print(f"\nTop {top_n_words} words from {filename}:")
                print(top_word_list)
            else:
                print(f"An error occurred during analysis of {filename}.")





!python.exe -m pip install --upgrade pip

!pip install transformers torch spacy scikit-learn numpy pandas tqdm matplotlib seaborn emoji
!python -m spacy download en_core_web_sm

import pandas as pd
import re
import emoji
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk import ngrams
from wordcloud import WordCloud
import nltk


# Download required NLTK data (run this once if you haven't already)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

def perform_eda(csv_file, column_name, num_topics=5):
    """
    Performs Exploratory Data Analysis (EDA) on a text column of a CSV file.

    Args:
        csv_file (str): Path to the CSV file.
        column_name (str): Name of the column containing the text data.
        num_topics (int): Number of topics to extract using topic modeling.
    """

    try:
        df = pd.read_csv(csv_file)
    except FileNotFoundError:
        print(f"Error: CSV file not found at {csv_file}")
        return
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return

    if column_name not in df.columns:
        print(f"Error: Column '{column_name}' not found in the CSV file.")
        return

    text_data = df[column_name].dropna().astype(str)  # Handle missing values, ensure string type

    print(f"Performing EDA on column: {column_name}")
    print(f"Number of documents: {len(text_data)}")
    print("-" * 40)

    # 1. Basic Textual Analysis
    print("Basic Textual Analysis:")
    print("-----------------------")

    # Document Length Distribution
    doc_lengths = text_data.apply(lambda x: len(x.split()))
    print(f"Mean document length: {doc_lengths.mean():.2f}")
    print(f"Median document length: {doc_lengths.median():.2f}")

    plt.figure(figsize=(10, 6))
    sns.histplot(doc_lengths, bins=50)
    plt.title("Document Length Distribution")
    plt.xlabel("Document Length (words)")
    plt.ylabel("Frequency")
    plt.show()

    # 2. Sentence Length Distribution
    print("\nSentence Length Distribution:")
    print("----------------------------")
    sentence_lengths = []
    for text in text_data:
        sentences = sent_tokenize(text)
        sentence_lengths.extend([len(word_tokenize(s)) for s in sentences])

    print(f"Mean sentence length: {sum(sentence_lengths) / len(sentence_lengths):.2f}")
    plt.figure(figsize=(10, 6))
    sns.histplot(sentence_lengths, bins=50)
    plt.title("Sentence Length Distribution")
    plt.xlabel("Sentence Length (words)")
    plt.ylabel("Frequency")
    plt.show()

    # 3. Word Frequency Analysis
    print("\nWord Frequency Analysis:")
    print("------------------------")

    stop_words = set(stopwords.words('english'))  # Use NLTK's stop words
    all_words = []
    for text in text_data:
        tokens = word_tokenize(text.lower())  # Tokenize and lowercase
        words = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove punctuation and stop words
        all_words.extend(words)

    word_counts = Counter(all_words)
    most_common_words = word_counts.most_common(20)
    print("Top 20 most common words (excluding stop words):")
    for word, count in most_common_words:
        print(f"{word}: {count}")

    # Visualize word frequencies
    words, counts = zip(*most_common_words)
    plt.figure(figsize=(12, 6))
    sns.barplot(x=list(words), y=list(counts))
    plt.xticks(rotation=90)
    plt.title("Top 20 Most Common Words")
    plt.xlabel("Word")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

    # 4. Word Cloud
    print("\nWord Cloud:")
    print("-------------")

    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title("Word Cloud")
    plt.show()

    # 5. N-gram Analysis (Bigrams)
    print("\nBigram Analysis:")
    print("-----------------")

    bigrams = ngrams(all_words, 2)
    bigram_counts = Counter(bigrams)
    most_common_bigrams = bigram_counts.most_common(10)

    print("Top 10 most common bigrams:")
    for bigram, count in most_common_bigrams:
        print(f"{bigram}: {count}")

    # 6. Emoji Identification
    print("\nEmoji Identification:")
    print("----------------------")

    all_emojis = []
    for text in text_data:
        emojis_found = [char for char in text if char in emoji.EMOJI_DATA]
        all_emojis.extend(emojis_found)

    if all_emojis:
        emoji_counts = Counter(all_emojis)
        print("Top 20 Most Frequent Emojis:")
        for emoji_char, count in emoji_counts.most_common(20):
            print(f"{emoji_char}: {count}")

        # Visualize Emoji Frequencies
        emojis, counts = zip(*emoji_counts.most_common(20))
        plt.figure(figsize=(12, 6))
        sns.barplot(x=list(emojis), y=list(counts))
        plt.xticks(fontsize=16)  # Make emojis larger on the x-axis
        plt.title("Top 20 Most Frequent Emojis")
        plt.xlabel("Emoji")
        plt.ylabel("Frequency")
        plt.tight_layout()
        plt.show()

    else:
        print("No emojis found in the text data.")

    print("\nEDA Completed.")


# Example Usage:  (Replace with your actual file and column name)
csv_file = r"C:\Users\I4C\Desktop\CyberGuardAI\db\crime_report.csv"   # Replace with the path to your CSV file
column_name = "crimeaditionalinfo"  # Replace with the name of the text column
perform_eda(csv_file, column_name, num_topics=4)  # Adjust num_topics as needed

"""# To test on Gpu install cupy"""

!pip install cupy-cuda12x

# This is a highly conditional example and might not provide any performance benefit.
# It assumes you have the RAPIDS environment set up correctly.
# Installation instructions: https://rapids.ai/start.html

try:
    import cudf
    import cupy as cp  # cuDF often uses CuPy for numerical operations
    import pandas as pd  # Still needed for other parts of the EDA
    import re
    import emoji
    from collections import Counter
    import matplotlib.pyplot as plt
    import seaborn as sns
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords
    from nltk import ngrams
    from wordcloud import WordCloud
    import nltk
    import multiprocessing

    # Download required NLTK data (run this once if you haven't already)
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')

    def perform_eda_cudf(csv_file, column_name, num_topics=5, num_processes=4):  # Added num_processes
        """
        Performs Exploratory Data Analysis (EDA) on a text column of a CSV file using cuDF.

        Args:
            csv_file (str): Path to the CSV file.
            column_name (str): Name of the column containing the text data.
            num_topics (int): Number of topics to extract using topic modeling.
            num_processes (int): Number of CPU processes to use for multiprocessing.
        """

        try:
            df = cudf.read_csv(csv_file)
        except FileNotFoundError:
            print(f"Error: CSV file not found at {csv_file}")
            return
        except Exception as e:
            print(f"Error reading CSV file: {e}")
            return

        if column_name not in df.columns:
            print(f"Error: Column '{column_name}' not found in the CSV file.")
            return

        text_data = df[column_name].dropna().astype(str).to_pandas()  # Convert to Pandas for NLP

        print(f"Performing EDA on column: {column_name}")
        print(f"Number of documents: {len(text_data)}")
        print("-" * 40)

        # 1. Basic Textual Analysis
        print("Basic Textual Analysis:")
        print("-----------------------")

        # Document Length Distribution
        doc_lengths = text_data.apply(lambda x: len(x.split()))
        print(f"Mean document length: {doc_lengths.mean():.2f}")
        print(f"Median document length: {doc_lengths.median():.2f}")

        plt.figure(figsize=(10, 6))
        sns.histplot(doc_lengths, bins=50)
        plt.title("Document Length Distribution")
        plt.xlabel("Document Length (words)")
        plt.ylabel("Frequency")
        plt.show()

        # 2. Sentence Length Distribution
        print("\nSentence Length Distribution:")
        print("----------------------------")
        sentence_lengths = []
        for text in text_data:
            sentences = sent_tokenize(text)
            sentence_lengths.extend([len(word_tokenize(s)) for s in sentences])

        print(f"Mean sentence length: {sum(sentence_lengths) / len(sentence_lengths):.2f}")
        plt.figure(figsize=(10, 6))
        sns.histplot(sentence_lengths, bins=50)
        plt.title("Sentence Length Distribution")
        plt.xlabel("Sentence Length (words)")
        plt.ylabel("Frequency")
        plt.show()

        # 3. Word Frequency Analysis
        print("\nWord Frequency Analysis:")
        print("------------------------")

        # Use multiprocessing to process text data
        def process_text(text):
            """Helper function to process text in parallel."""
            tokens = word_tokenize(text.lower())
            stop_words = set(stopwords.words('english'))
            words = [word for word in tokens if word.isalnum() and word not in stop_words]
            return words

        with multiprocessing.Pool(processes=num_processes) as pool:
            all_words_nested = pool.map(process_text, text_data)

        # Flatten the list of lists
        all_words = [word for sublist in all_words_nested for word in sublist]

        word_counts = Counter(all_words)
        most_common_words = word_counts.most_common(20)
        print("Top 20 most common words (excluding stop words):")
        for word, count in most_common_words:
            print(f"{word}: {count}")

        # Visualize word frequencies
        words, counts = zip(*most_common_words)
        plt.figure(figsize=(12, 6))
        sns.barplot(x=list(words), y=list(counts))
        plt.xticks(rotation=90)
        plt.title("Top 20 Most Common Words")
        plt.xlabel("Word")
        plt.ylabel("Frequency")
        plt.tight_layout()
        plt.show()

        # 4. Word Cloud
        print("\nWord Cloud:")
        print("-------------")

        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(all_words))
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title("Word Cloud")
        plt.show()

        # 5. N-gram Analysis (Bigrams)
        print("\nBigram Analysis:")
        print("-----------------")

        bigrams = ngrams(all_words, 2)
        bigram_counts = Counter(bigrams)
        most_common_bigrams = bigram_counts.most_common(10)

        print("Top 10 most common bigrams:")
        for bigram, count in most_common_bigrams:
            print(f"{bigram}: {count}")

        # 6. Emoji Identification
        print("\nEmoji Identification:")
        print("----------------------")

        all_emojis = []
        for text in text_data:
            emojis_found = [char for char in text if char in emoji.EMOJI_DATA]
            all_emojis.extend(emojis_found)

        if all_emojis:
            emoji_counts = Counter(all_emojis)
            print("Top 20 Most Frequent Emojis:")
            for emoji_char, count in emoji_counts.most_common(20):
                print(f"{emoji_char}: {count}")

            # Visualize Emoji Frequencies
            emojis, counts = zip(*emoji_counts.most_common(20))
            plt.figure(figsize=(12, 6))
            sns.barplot(x=list(emojis), y=list(counts))
            plt.xticks(fontsize=16)  # Make emojis larger on the x-axis
            plt.title("Top 20 Most Frequent Emojis")
            plt.xlabel("Emoji")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plt.show()

        else:
            print("No emojis found in the text data.")

        print("\nEDA Completed.")

    # Example Usage:  (Replace with your actual file and column name)
    csv_file = r"C:\Users\I4C\Desktop\CyberGuardAI\db\crime_report.csv"   # Replace with the path to your CSV file
    column_name = "crimeaditionalinfo"  # Replace with the name of the text column
    perform_eda_cudf(csv_file, column_name, num_topics=4, num_processes=multiprocessing.cpu_count())  # Adjust num_topics as needed
except ImportError:
    print("cuDF is not installed. Please install RAPIDS to use GPU acceleration for CSV loading (but NLP will still be CPU-based).")

"""# Data Cleaning"""

import os
import pandas as pd
import re
import csv
folder_path= r'C:\Users\I4C\Desktop\CyberGuardAI\db'
pattern = r'[^a-zA-Z0-9\s]'
def clean_column(data, column_name):
    if column_name in data.columns:

        data[column_name]= data[column_name].apply(lambda x: re.sub(pattern, '', str(x)))
        data[column_name] = data[column_name].str.lower().str.strip()
    else:
        print(f"Column '{column_name}' not found!")
        return data



for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path, filename)
    if filename.endswith(('.csv', '.xlsx', '.ods')):
        print(f"Processing file: {filename}")
        try:
            if filename.endswith('.csv'):
                df = pd.read_csv(file_path)
            elif filename.endswith('.xlsx'):
                df = pd.read_excel(file_path, engine='openpyxl')
            elif filename.endswith('.ods'):
                df = pd.read_excel(file_path, engine='odf')
                column_name = 'crimeaditionalinfo'
            elif column_name in df.columns:
                df = clean_column(df, column_name)
                df[column_name].fillna('Unknown Crime Description', inplace=True)
                df.dropna(how='all', inplace=True)
                df.drop_duplicates(subset=[column_name], inplace=True)
                cleaned_file_path = os.path.join(folder_path, f"cleaned_{filename.replace('.xlsx','.csv').replace('.ods','.csv')}")
                os.makedirs(os.path.dirname(cleaned_file_path), exist_ok=True)
                df.to_csv(cleaned_file_path, index=false)
                print(f"Saved cleaned file: {cleaned_file_path}")
            else:
                print(f"Column '{column_name}' not found in {filename}. Skipping cleaning for this file. ")
        except Exception as e:
            print(f"Error processing file {filename}: {e}")

"""the new files mow contain only cleaned text and can be further used for labelinng

# Model 1

## Spacy Model

### now i created a model using spacy run it in an env with all the files installed most of the time spacy got corrpt either use tutorial-env on vm which might take long time or trch on gpu which will take around 4 hours
"""

# Import required packages
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import spacy
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple
import re
from datetime import datetime
import logging
from functools import lru_cache
from flask import Flask, request, jsonify
from threading import Thread
import os

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)

# Set CUDA device (if available)
if torch.cuda.is_available():
    device = torch.device(f"cuda:{2}" if torch.cuda.device_count() > 2 else "cuda:0")
    logger.info(f"Using CUDA device: {device}")
else:
    device = torch.device("cpu")
    logger.info("CUDA not available, using CPU")


class RiskAnalyzer:
    """Analyzes text and assigns risk levels"""
    def __init__(self):
        # Keywords for different risk levels
        self.risk_keywords = {
            'high_risk': [  # Level 3
                'suicide', 'kill', 'murder', 'threat', 'die', 'death',
                'minor', 'child', 'children', 'cyber bullying', 'cyberbully',
                'blackmail', 'extortion', 'abuse', 'sexual', 'rape', 'kidnap',
                'life threat', 'revenge', 'nude', 'explicit', 'torture',
                'bomb', 'attack', 'harassment', 'stalking', 'private photo',
                'physical', 'violence', 'danger', 'weapon'
            ],
            'medium_risk': [  # Level 2
                'fraud', 'scam', 'hack', 'stolen', 'theft', 'upi',
                'bank', 'credit card', 'debit card', 'money', 'payment',
                'account', 'password', 'compromise', 'cryptocurrency',
                'bitcoin', 'investment', 'otp', 'kyc', 'aadhaar',
                'pan card', 'document', 'identity theft', 'lost money',
                'unauthorized', 'transaction', 'financial'
            ],
            'low_risk': [  # Level 1
                'spam', 'advertisement', 'fake profile', 'friend request',
                'social media', 'connection', 'follow', 'unknown', 'suspicious',
                'fake account', 'wrong', 'mistake', 'error', 'help',
                'question', 'information', 'advice'
            ]
        }

    def analyze_text(self, text: str) -> Dict:
        """
        Analyze text and return risk assessment
        """
        # Handle non-string inputs
        if not isinstance(text, str):
            if pd.isna(text):
                return {
                    'risk_level': 1,
                    'reasons': ['Empty text'],
                    'description': 'Empty or null input'
                }
            text = str(text)

        text = text.lower()

        # Check for high risk keywords (Level 3)
        high_risk_matches = [word for word in self.risk_keywords['high_risk']
                           if word in text]
        if high_risk_matches:
            return {
                'risk_level': 3,
                'reasons': high_risk_matches,
                'description': 'High Risk - Immediate Attention Required'
            }

        # Check for medium risk keywords (Level 2)
        medium_risk_matches = [word for word in self.risk_keywords['medium_risk']
                             if word in text]
        if medium_risk_matches:
            return {
                'risk_level': 2,
                'reasons': medium_risk_matches,
                'description': 'Medium Risk - Priority Processing'
            }

        # Default to low risk (Level 1)
        low_risk_matches = [word for word in self.risk_keywords['low_risk']
                          if word in text]
        return {
            'risk_level': 1,
            'reasons': low_risk_matches or ['general complaint'],
            'description': 'Low Risk - Regular Processing'
        }

class TraditionalProcessor:
    """Traditional ML-based approach using spaCy"""
    def __init__(self):
        self.nlp = self.load_spacy_model()
        self.risk_analyzer = RiskAnalyzer()

    @staticmethod
    @lru_cache(maxsize=1)
    def load_spacy_model():
        """Load and cache spaCy model"""
        try:
            return spacy.load("en_core_web_sm")
        except Exception as e:
            logger.error(f"Failed to load spaCy model: {e}")
            raise

    def process_text(self, text: str) -> Dict:
        """Process text using traditional NLP approach"""
        # Handle non-string inputs
        if not isinstance(text, str):
            if pd.isna(text):
                text = ""
            else:
                text = str(text)

        start_time = time.time()

        try:
            # Process text with spaCy
            doc = self.nlp(text)

            # Extract entities
            entities = [(ent.text, ent.label_) for ent in doc.ents]

            # Get risk assessment
            risk_assessment = self.risk_analyzer.analyze_text(text)

            return {
                'entities': entities,
                'risk_assessment': risk_assessment,
                'processing_time': time.time() - start_time,
                'method': 'traditional'
            }
        except Exception as e:
            logger.error(f"Error processing text with traditional approach: {e}")
            return {
                'entities': [],
                'risk_assessment': {
                    'risk_level': 1,
                    'reasons': ['Processing error'],
                    'description': 'Error during processing'
                },
                'processing_time': time.time() - start_time,
                'method': 'traditional'
            }
class CybercrimeAnalysisSystem:
    """Main system combining both traditional and LLM approaches"""
    def __init__(self):
        self.traditional_processor = TraditionalProcessor()
        self.llm_processor = LLMProcessor()
        self.use_gpu = torch.cuda.is_available()

    def process_text(self, text: str) -> Dict:
        """Process text using both approaches"""
        try:
            trad_result = self.traditional_processor.process_text(text)
            llm_result = self.llm_processor.process_text(text)

            # Combine results
            result = {
                'text': text,
                'traditional_risk_level': trad_result['risk_assessment']['risk_level'],
                'llm_risk_level': llm_result['risk_assessment']['risk_level'],
                'traditional_processing_time': trad_result['processing_time'],
                'llm_processing_time': llm_result['processing_time'],
                'risk_reasons': trad_result['risk_assessment']['reasons'],
                'risk_description': trad_result['risk_assessment']['description']
            }

            return result

        except Exception as e:
            logger.error(f"Error processing text: {e}")
            return {
                'text': text,
                'traditional_risk_level': 1,
                'llm_risk_level': 1,
                'traditional_processing_time': 0,
                'llm_processing_time': 0,
                'risk_reasons': ['Processing error'],
                'risk_description': 'Error during processing'
            }


    def process_dataset(self, df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
        """Process entire dataset with both approaches"""
        results = []

        # Clean the input data
        df['crimeaditionalinfo'] = df['crimeaditionalinfo'].fillna("")
        df['crimeaditionalinfo'] = df['crimeaditionalinfo'].astype(str)

        # Process in batches
        for start_idx in tqdm(range(0, len(df), batch_size)):
            batch_df = df.iloc[start_idx:start_idx + batch_size]

            for idx, row in batch_df.iterrows():
                text = row['crimeaditionalinfo']

                try:
                    # Process with both approaches
                    trad_result = self.traditional_processor.process_text(text)
                    llm_result = self.llm_processor.process_text(text)

                    # Combine results
                    result = {
                        'text': text,
                        # 'category': row['category'],
                        # 'sub_category': row['sub_category'],
                        'traditional_risk_level': trad_result['risk_assessment']['risk_level'],
                        'llm_risk_level': llm_result['risk_assessment']['risk_level'],
                        'traditional_processing_time': trad_result['processing_time'],
                        'llm_processing_time': llm_result['processing_time'],
                        'risk_reasons': trad_result['risk_assessment']['reasons'],
                        'risk_description': trad_result['risk_assessment']['description']
                    }

                    results.append(result)
                except Exception as e:
                    logger.error(f"Error processing row {idx}: {e}")
                    results.append({
                        'text': text,
                        # 'category': row['category'],
                        # 'sub_category': row['sub_category'],
                        'traditional_risk_level': 1,
                        'llm_risk_level': 1,
                        'traditional_processing_time': 0,
                        'llm_processing_time': 0,
                        'risk_reasons': ['Processing error'],
                        'risk_description': 'Error during processing'
                    })

        return pd.DataFrame(results)

def visualize_results(df: pd.DataFrame) -> None:
    """Create visualizations for analysis results"""
    if df.empty:
        logger.warning("Cannot create visualizations: DataFrame is empty")
        return

    plt.style.use('seaborn')
    fig = plt.figure(figsize=(20, 15))

    # 1. Risk Level Distribution
    plt.subplot(2, 2, 1)
    risk_dist = df['traditional_risk_level'].value_counts().sort_index()
    plt.bar(risk_dist.index, risk_dist.values)
    plt.title('Risk Level Distribution')
    plt.xlabel('Risk Level')
    plt.ylabel('Number of Cases')

    # 2. Processing Time Comparison
    plt.subplot(2, 2, 2)
    times_data = {
        'Traditional': df['traditional_processing_time'],
        'LLM': df['llm_processing_time']
    }
    plt.boxplot([times_data['Traditional'], times_data['LLM']], labels=['Traditional', 'LLM'])
    plt.title('Processing Time Comparison')
    plt.ylabel('Time (seconds)')

    # 3. Risk Level by Category
    plt.subplot(2, 2, 3)
    risk_cat = pd.crosstab(df['category'], df['traditional_risk_level'])
    sns.heatmap(risk_cat, annot=True, cmap='YlOrRd')
    plt.title('Risk Level by Category')

    # 4. Processing Method Agreement
    plt.subplot(2, 2, 4)
    agreement = (df['traditional_risk_level'] == df['llm_risk_level']).mean() * 100
    plt.pie([agreement, 100-agreement], labels=['Agree', 'Disagree'], autopct='%1.1f%%')
    plt.title('Processing Method Agreement')

    plt.tight_layout()
    plt.show()

# API Endpoint
@app.route('/analyze', methods=['POST'])
def analyze_text_endpoint():
    """Endpoint to analyze text using CybercrimeAnalysisSystem"""
    try:
        data = request.get_json()
        text = data.get('text')

        if not text:
            return jsonify({"error": "Text is required"}), 400

        # Initialize system
        system = CybercrimeAnalysisSystem()

        # Process text
        result = system.process_text(text)

        return jsonify(result), 200

    except Exception as e:
        logger.error(f"Error processing request: {e}")
        return jsonify({"error": str(e)}), 500

def main():
    try:
        # Load dataset
        logger.info("Loading dataset...")
        df = pd.read_csv(r'/home/i4cuser8/prj/db/crime_report.csv') # Changed to only load one CSV change the path accordingly where you are using this code wheather on cpu or gpu

        # Split data into 80% train and 20% test
        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) # Added split

        # Initialize system
        logger.info("Initializing system...")
        system = CybercrimeAnalysisSystem()

        # Process training data
        logger.info("Processing training data...")
        train_results = system.process_dataset(train_df)

        # Process test data
        logger.info("Processing test data...")
        test_results = system.process_dataset(test_df)

        # Visualize results
        logger.info("Generating visualizations...")
        visualize_results(train_results)

        # Save processed data
        train_results.to_csv('train_processed.csv', index=False)
        test_results.to_csv('test_processed.csv', index=False)

        # Print summary statistics
        logger.info("\nSummary Statistics:")
        logger.info("-" * 50)
        logger.info(f"Total cases processed: {len(train_results) + len(test_results)}")
        logger.info("\nRisk Level Distribution (Training Data):")
        logger.info(train_results['traditional_risk_level'].value_counts().sort_index())
        logger.info("\nAverage Processing Times:")
        logger.info(f"Traditional: {train_results['traditional_processing_time'].mean():.4f} seconds")
        logger.info(f"LLM: {train_results['llm_processing_time'].mean():.4f} seconds")

    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise

# Run Flask app in a separate thread
def run_app():
    app.run(debug=False, host='0.0.0.0', port=5000)

if __name__ == "__main__":
    # Start Flask app in a separate thread
    app_thread = Thread(target=run_app)
    app_thread.daemon = True  # Daemonize thread so it exits when main thread exits
    app_thread.start()

    # Run main function
    main()

"""# Model 2

### My Custom model with CNN and Custom Tranformer kindly prefer using gpu if pushe3d over locally will crash your pc and tweek the code just to give access to all the gpu's i couldnt do so becuase i was only assigned 1 gpu and various gpu crashes make
"""

"""
Automated Crime Classification Pipeline for Hindi-English Text
Complete end-to-end implementation with PyTorch
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import umap
import re
import string
import json
import logging
import time
from datetime import datetime
from tqdm import tqdm
import warnings
from collections import Counter
import pickle
from torch.utils.tensorboard import SummaryWriter
os.environ['OPENBLAS_NUM_THREADS']='4'
# Set up logging
def setup_logging(output_dir='output'):
    os.makedirs(output_dir, exist_ok=True)
    log_file = os.path.join(output_dir, "crime_classifier.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger('CrimeClassifier')
    warnings.filterwarnings('ignore')
    return logger

# Initialize logger
logger = setup_logging()

# Get device for PyTorch
def get_device():
    if torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = torch.cuda.get_device_name(0)
        logger.info(f"Using GPU: {device_name}")
    else:
        device = torch.device('cpu')
        logger.info("Using CPU")
    return device

class DataPreprocessor:
    """
    Handles preprocessing of bilingual (Hindi-English) text data
    """
    def __init__(self, max_length=512, min_freq=5):
        self.max_length = max_length
        self.min_freq = min_freq
        self.vocab = None
        self.label_encoder = None
        self.char_counter = None
        self.vectorizer = None
        logger.info("Initialized DataPreprocessor")

    def _clean_text(self, text):
        """Clean text while preserving Hindi characters"""
        if not isinstance(text, str):
            text = str(text)
        # Remove URLs
        text = re.sub(r'http\S+', '', text)
        # Remove special chars but keep Hindi Unicode range (0900-097F)
        text = re.sub(r'[^\u0900-\u097Fa-zA-Z0-9\s]', ' ', text)
        # Remove extra whitespace
        text = ' '.join(text.split())
        return text.lower()

    def _tokenize_bilingual(self, text):
        """Tokenize Hindi-English text"""
        tokens = text.split()
        tokens = [token.strip(string.punctuation) for token in tokens]
        tokens = [token for token in tokens if token]
        return tokens

    def build_char_vocabulary(self, texts):
        """Build character-level vocabulary"""
        logger.info("Building character vocabulary...")
        self.char_counter = Counter()
        for text in tqdm(texts):
            self.char_counter.update(self._clean_text(text))

        # Filter by frequency and create vocab
        vocab = {'<PAD>': 0, '<UNK>': 1}
        idx = 2
        for char, count in self.char_counter.items():
            if count >= self.min_freq:
                vocab[char] = idx
                idx += 1

        self.vocab = vocab
        logger.info(f"Built vocabulary with {len(self.vocab)} characters")
        return self.vocab

    def create_tfidf_vectors(self, texts):
        """Create TF-IDF vectors for clustering"""
        logger.info("Creating TF-IDF vectors...")
        cleaned_texts = [self._clean_text(text) for text in tqdm(texts)]

        self.vectorizer = TfidfVectorizer(
            tokenizer=self._tokenize_bilingual,
            ngram_range=(1, 2),
            max_features=15000,
            min_df=5
        )

        vectors = self.vectorizer.fit_transform(cleaned_texts)
        logger.info(f"Created TF-IDF vectors with shape {vectors.shape}")
        return vectors

    def char_encode_texts(self, texts):
        """Encode texts as character sequences"""
        if self.vocab is None:
            raise ValueError("Vocabulary not built. Call build_char_vocabulary first.")

        sequences = np.zeros((len(texts), self.max_length), dtype=np.int32)
        for i, text in enumerate(tqdm(texts)):
            cleaned = self._clean_text(text)
            for j, char in enumerate(cleaned[:self.max_length]):
                sequences[i, j] = self.vocab.get(char, self.vocab['<UNK>'])

        return sequences

    def encode_labels(self, labels):
        """Encode cluster labels for training"""
        self.label_encoder = LabelEncoder()
        encoded_labels = self.label_encoder.fit_transform(labels)
        return encoded_labels

    def save(self, path="preprocessor.pkl"):
        """Save preprocessor state"""
        with open(path, 'wb') as f:
            pickle.dump({
                'vocab': self.vocab,
                'label_encoder': self.label_encoder,
                'vectorizer': self.vectorizer,
                'char_counter': self.char_counter,
                'max_length': self.max_length,
                'min_freq': self.min_freq
            }, f)
        logger.info(f"Saved preprocessor to {path}")

    def load(self, path="preprocessor.pkl"):
        """Load preprocessor state"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.vocab = data['vocab']
            self.label_encoder = data['label_encoder']
            self.vectorizer = data['vectorizer']
            self.char_counter = data['char_counter']
            self.max_length = data['max_length']
            self.min_freq = data['min_freq']
        logger.info(f"Loaded preprocessor from {path}")


class AutoClusterer:
    """
    Automatically clusters text data and determines optimal cluster count
    """
    def __init__(self, min_clusters=30, max_clusters=70, random_state=42):
        self.min_clusters = min_clusters
        self.max_clusters = max_clusters
        self.random_state = random_state
        self.best_n_clusters = None
        self.best_model = None
        self.cluster_labels = None
        self.cluster_centers = None
        self.reduced_vectors = None
        logger.info(f"Initialized AutoClusterer with range {min_clusters}-{max_clusters}")

    def _reduce_dimensions(self, X, n_components=100):
        """Reduce dimensionality for faster clustering"""
        logger.info(f"Reducing dimensions to {n_components}")
        svd = TruncatedSVD(n_components=n_components, random_state=self.random_state)
        reduced = svd.fit_transform(X)
        logger.info(f"Explained variance: {svd.explained_variance_ratio_.sum():.4f}")
        self.reduced_vectors = reduced
        return reduced

    def find_optimal_clusters(self, X):
        """Find optimal number of clusters using multiple metrics"""
        logger.info("Finding optimal number of clusters...")

        # Reduce dimensions for faster computation
        X_reduced = self._reduce_dimensions(X)

        # Metrics to store
        silhouette_scores = []
        ch_scores = []

        # Test different cluster counts
        for n_clusters in tqdm(range(self.min_clusters, self.max_clusters + 1, 2)):
            kmeans = KMeans(
                n_clusters=n_clusters,
                random_state=self.random_state,
                n_init=10
            )
            labels = kmeans.fit_predict(X_reduced)

            # Calculate clustering metrics
            sil_score = silhouette_score(X_reduced, labels)
            ch_score = calinski_harabasz_score(X_reduced, labels)

            silhouette_scores.append(sil_score)
            ch_scores.append(ch_score)

            logger.info(f"Clusters: {n_clusters}, Silhouette: {sil_score:.4f}, CH: {ch_score:.4f}")

        # Normalize scores
        norm_silhouette = np.array(silhouette_scores) / max(silhouette_scores)
        norm_ch = np.array(ch_scores) / max(ch_scores)

        # Combine metrics (weighted average)
        combined_score = 0.6 * norm_silhouette + 0.4 * norm_ch
        best_idx = np.argmax(combined_score)
        self.best_n_clusters = self.min_clusters + 2 * best_idx

        logger.info(f"Optimal number of clusters: {self.best_n_clusters}")

        # Plot scores
        self._plot_cluster_scores(silhouette_scores, ch_scores, combined_score)

        return self.best_n_clusters

    def _plot_cluster_scores(self, silhouette_scores, ch_scores, combined_scores):
        """Plot clustering metrics"""
        cluster_range = list(range(self.min_clusters, self.max_clusters + 1, 2))

        plt.figure(figsize=(12, 8))

        plt.subplot(3, 1, 1)
        plt.plot(cluster_range, silhouette_scores, 'o-', color='blue')
        plt.title('Silhouette Score')
        plt.xlabel('Number of Clusters')
        plt.grid(True)

        plt.subplot(3, 1, 2)
        plt.plot(cluster_range, ch_scores, 'o-', color='green')
        plt.title('Calinski-Harabasz Score')
        plt.xlabel('Number of Clusters')
        plt.grid(True)

        plt.subplot(3, 1, 3)
        plt.plot(cluster_range, combined_scores, 'o-', color='red')
        plt.title('Combined Score (Weighted Average)')
        plt.xlabel('Number of Clusters')
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('cluster_scores.png')
        logger.info("Saved cluster score plots to cluster_scores.png")

    def perform_clustering(self, X):
        """Perform clustering with optimal parameters"""
        if self.best_n_clusters is None:
            self.find_optimal_clusters(X)

        logger.info(f"Performing clustering with {self.best_n_clusters} clusters")

        if self.reduced_vectors is None:
            X_reduced = self._reduce_dimensions(X)
        else:
            X_reduced = self.reduced_vectors

        kmeans = KMeans(
            n_clusters=self.best_n_clusters,
            random_state=self.random_state,
            n_init=20,
            max_iter=500
        )

        self.cluster_labels = kmeans.fit_predict(X_reduced)
        self.best_model = kmeans
        self.cluster_centers = kmeans.cluster_centers_

        logger.info("Clustering complete")
        return self.cluster_labels

    def visualize_clusters(self, X, output_file='cluster_visualization.png'):
        """Visualize clusters using UMAP"""
        if self.cluster_labels is None:
            raise ValueError("Clustering not performed yet")

        logger.info("Visualizing clusters with UMAP...")

        # Use reduced vectors if available
        if self.reduced_vectors is not None:
            X_vis = self.reduced_vectors
        else:
            X_vis = self._reduce_dimensions(X)

        # Further reduce to 2D for visualization
        reducer = umap.UMAP(random_state=self.random_state)
        X_2d = reducer.fit_transform(X_vis)

        # Plot clusters
        plt.figure(figsize=(16, 12))
        scatter = plt.scatter(
            X_2d[:, 0], X_2d[:, 1],
            c=self.cluster_labels,
            cmap='tab20',
            alpha=0.7,
            s=10
        )
        plt.colorbar(scatter)
        plt.title(f'UMAP Visualization of {self.best_n_clusters} Clusters')
        plt.savefig(output_file, dpi=300)
        plt.close()

        logger.info(f"Saved cluster visualization to {output_file}")


class CrimeDataset(Dataset):
    """
    PyTorch Dataset for crime classification
    """
    def __init__(self, sequences, labels=None):
        self.sequences = torch.tensor(sequences, dtype=torch.long)
        if labels is not None:
            self.labels = torch.tensor(labels, dtype=torch.long)
        else:
            self.labels = None

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        if self.labels is not None:
            return self.sequences[idx], self.labels[idx]
        else:
            return self.sequences[idx]


class SelfAttention(nn.Module):
    """
    Multi-head self-attention layer
    """
    def __init__(self, embed_dim, num_heads, dropout=0.1):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-6)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        attn_output, _ = self.multihead_attn(x, x, x)
        out = self.dropout(attn_output)
        return self.layer_norm(x + out)


class FeedForward(nn.Module):
    """
    Position-wise feed-forward network
    """
    def __init__(self, embed_dim, ff_dim, dropout=0.1):
        super().__init__()
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim)
        )
        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-6)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        ff_output = self.ff(x)
        out = self.dropout(ff_output)
        return self.layer_norm(x + out)


class TransformerBlock(nn.Module):
    """
    Transformer block: self-attention + feed-forward
    """
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.attention = SelfAttention(embed_dim, num_heads, dropout)
        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout)

    def forward(self, x):
        x = self.attention(x)
        return self.feed_forward(x)


class ComplexBilingualModel(nn.Module):
    """
    Complex neural network model for bilingual text classification
    """
    def __init__(self, vocab_size, num_classes, max_length=512, embed_dims=None):
        super().__init__()
        if embed_dims is None:
            self.embed_dims = [256, 384, 512]
        else:
            self.embed_dims = embed_dims

        self.vocab_size = vocab_size
        self.num_classes = num_classes
        self.max_length = max_length

        # Create embeddings of different dimensions
        self.embeddings = nn.ModuleList([
            nn.Embedding(vocab_size, dim, padding_idx=0)
            for dim in self.embed_dims
        ])

        # Convolutional layers for each embedding
        self.conv_blocks = nn.ModuleList()
        for dim in self.embed_dims:
            # Multi-scale convolutions for each embedding
            block = nn.ModuleList()
            for kernel_size in [3, 5, 7]:
                conv = nn.Sequential(
                    nn.Conv1d(dim, dim, kernel_size, padding=kernel_size//2),
                    nn.GroupNorm(8, dim),  # Use GroupNorm instead of LayerNorm for Conv1d
                    nn.GELU(),
                    nn.Conv1d(dim, dim, kernel_size, padding=kernel_size//2),
                    nn.GroupNorm(8, dim),
                    nn.GELU()
                )
                block.append(conv)
            self.conv_blocks.append(block)

        # Dimension reduction after concatenation
        self.dim_reducers = nn.ModuleList([
            nn.Conv1d(dim * 3, dim, 1)
            for dim in self.embed_dims
        ])

        # Transformer blocks for each embedding branch
        self.transformer_blocks = nn.ModuleList()
        for dim in self.embed_dims:
            transformers = nn.Sequential(*[
                TransformerBlock(
                    embed_dim=dim,
                    num_heads=8,
                    ff_dim=dim*4,
                    dropout=0.1
                ) for _ in range(4)
            ])
            self.transformer_blocks.append(transformers)

        # Bidirectional LSTM layers
        total_dim = sum(self.embed_dims)
        self.lstm = nn.LSTM(
            input_size=total_dim,
            hidden_size=512,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.2
        )

        # MLP head with residual connection
        self.mlp = nn.Sequential(
            nn.Linear(512*2, 1024),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(1024, 1024),
            nn.GELU(),
            nn.Dropout(0.2)
        )

        # Skip connection
        self.skip_projection = nn.Linear(512*2, 1024)

        # Output layer
        self.classifier = nn.Linear(1024, num_classes)

    def forward(self, x):
        # Process each embedding dimension
        branch_outputs = []

        for i, dim in enumerate(self.embed_dims):
            # Get embedding
            embedded = self.embeddings[i](x)

            # Apply multi-scale convolution blocks
            conv_outputs = []
            for conv in self.conv_blocks[i]:
                # Transpose for convolution (batch, seq_len, embed) -> (batch, embed, seq_len)
                conv_in = embedded.transpose(1, 2)
                conv_out = conv(conv_in)
                conv_outputs.append(conv_out)

            # Concatenate and reduce conv outputs
            conv_cat = torch.cat(conv_outputs, dim=1)
            conv_reduced = self.dim_reducers[i](conv_cat)

            # Transpose back for transformer (batch, embed, seq_len) -> (batch, seq_len, embed)
            transformer_in = conv_reduced.transpose(1, 2)

            # Apply transformer blocks
            transformed = self.transformer_blocks[i](transformer_in)

            # Global pooling
            pooled = torch.mean(transformed, dim=1)
            branch_outputs.append(pooled)

        # Concatenate all branch outputs
        concat = torch.cat(branch_outputs, dim=1)

        # Reshape for LSTM: (batch, features) -> (batch, 1, features)
        lstm_in = concat.unsqueeze(1)

        # LSTM processing
        lstm_out, _ = self.lstm(lstm_in)
        lstm_out = lstm_out.squeeze(1)

        # MLP with residual connection
        mlp_out = self.mlp(lstm_out)
        skip = self.skip_projection(lstm_out)
        combined = mlp_out + skip

        # Classification
        logits = self.classifier(combined)
        return logits


class ModelTrainer:
    """
    Handles model training and evaluation
    """
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.best_val_acc = 0.0
        self.writer = SummaryWriter(log_dir=f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
        logger.info(f"Initialized ModelTrainer with {model.__class__.__name__}")

    def train(self, train_loader, val_loader, epochs=30, lr=5e-5, weight_decay=0.01):
        """Train the model"""
        logger.info(f"Starting training for {epochs} epochs")

        # Move model to device
        self.model.to(self.device)

        # Initialize optimizer
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999),
            eps=1e-7
        )

        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=0.5,
            patience=3,
            verbose=True
        )

        # Loss function
        criterion = nn.CrossEntropyLoss()

        # Mixed precision scaler
        scaler = GradScaler()

        # Training and validation metrics
        metrics_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'val_top3_acc': [],
            'val_top5_acc': []
        }

        # Start training
        start_time = time.time()
        for epoch in range(epochs):
            epoch_start = time.time()

            # Training phase
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0

            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
            for batch_idx, (inputs, targets) in enumerate(train_pbar):
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # Zero gradients
                optimizer.zero_grad()

                # Forward pass with mixed precision
                with autocast():
                    outputs = self.model(inputs)
                    loss = criterion(outputs, targets)

                # Backward pass with gradient scaling
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()

                # Update metrics
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()

                # Update progress bar
                avg_loss = train_loss / (batch_idx + 1)
                acc = 100. * train_correct / train_total
                train_pbar.set_postfix({
                    'loss': f"{avg_loss:.4f}",
                    'acc': f"{acc:.2f}%"
                })

            # Calculate epoch metrics
            train_loss = train_loss / len(train_loader)
            train_acc = 100. * train_correct / train_total

            # Validation phase
            val_loss, val_acc, val_top3_acc, val_top5_acc = self.evaluate(val_loader, criterion)

            # Update learning rate
            scheduler.step(val_acc)

            # Save best model
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.save_model('best_model.pt')
                logger.info(f"Saved new best model with validation accuracy: {val_acc:.2f}%")

            # Update metrics history
            metrics_history['train_loss'].append(train_loss)
            metrics_history['train_acc'].append(train_acc)
            metrics_history['val_loss'].append(val_loss)
            metrics_history['val_acc'].append(val_acc)
            metrics_history['val_top3_acc'].append(val_top3_acc)
            metrics_history['val_top5_acc'].append(val_top5_acc)

            # Log to tensorboard
            self.writer.add_scalar('Loss/train', train_loss, epoch)
            self.writer.add_scalar('Loss/val', val_loss, epoch)
            self.writer.add_scalar('Accuracy/train', train_acc, epoch)
            self.writer.add_scalar('Accuracy/val', val_acc, epoch)
            self.writer.add_scalar('Accuracy/val_top3', val_top3_acc, epoch)
            self.writer.add_scalar('Accuracy/val_top5', val_top5_acc, epoch)
            self.writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)

            # Log epoch summary
            epoch_time = time.time() - epoch_start
            logger.info(
                f"Epoch {epoch+1}/{epochs} - "
                f"Time: {epoch_time:.2f}s - "
                f"Train Loss: {train_loss:.4f} - "
                f"Train Acc: {train_acc:.2f}% - "
                f"Val Loss: {val_loss:.4f} - "
                f"Val Acc: {val_acc:.2f}% - "
                f"Val Top-3: {val_top3_acc:.2f}% - "
                f"Val Top-5: {val_top5_acc:.2f}%"
            )

            # Early stopping check (after 10 epochs)
            if epoch > 10 and all(metrics_history['val_acc'][-1] <= val_acc for val_acc in metrics_history['val_acc'][-6:-1]):
                logger.info("Early stopping: No improvement for 5 epochs")
                break

        # Training complete
        total_time = time.time() - start_time
        logger.info(f"Training completed in {total_time/60:.2f} minutes")

        # Plot training history
        self.plot_training_history(metrics_history)

        # Close tensorboard writer
        self.writer.close()

        return metrics_history

    def evaluate(self, val_loader, criterion):
        """Evaluate the model"""
        self.model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        top3_correct = 0
        top5_correct = 0

        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc="Validation")
            for inputs, targets in val_pbar:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                with autocast():
                    outputs = self.model(inputs)
                    loss = criterion(outputs, targets)

                val_loss += loss.item()

                # Top-1 accuracy
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()

                # Top-3 accuracy
                _, top3_preds = outputs.topk(3, 1)
                batch_size = targets.size(0)
                for i in range(batch_size):
                    if targets[i] in top3_preds[i]:
                        top3_correct += 1

                # Top-5 accuracy
                _, top5_preds = outputs.topk(5, 1)
                for i in range(batch_size):
                    if targets[i] in top5_preds[i]:
                        top5_correct += 1

                # Update progress bar
                val_pbar.set_postfix({
                    'loss': f"{val_loss / (val_pbar.n + 1):.4f}",
                    'acc': f"{100. * val_correct / val_total:.2f}%"
                })

        # Calculate final metrics
        val_loss = val_loss / len(val_loader)
        val_acc = 100. * val_correct / val_total
        val_top3_acc = 100. * top3_correct / val_total
        val_top5_acc = 100. * top5_correct / val_total

        logger.info(
            f"Validation - "
            f"Loss: {val_loss:.4f}, "
            f"Acc: {val_acc:.2f}%, "
            f"Top-3: {val_top3_acc:.2f}%, "
            f"Top-5: {val_top5_acc:.2f}%"
        )

        return val_loss, val_acc, val_top3_acc, val_top5_acc

    def save_model(self, file_path):
        """Save model weights and configuration"""
        model_info = {
            'model_state_dict': self.model.state_dict(),
            'vocab_size': self.model.vocab_size,
            'num_classes': self.model.num_classes,
            'max_length': self.model.max_length,
            'embed_dims': self.model.embed_dims,
            'val_accuracy': self.best_val_acc
        }
        torch.save(model_info, file_path)

    def load_model(self, file_path):
        """Load model weights"""
        model_info = torch.load(file_path, map_location=self.device)
        self.model.load_state_dict(model_info['model_state_dict'])
        self.best_val_acc = model_info['val_accuracy']
        logger.info(f"Loaded model from {file_path} with validation accuracy: {self.best_val_acc:.2f}%")
        return self.model

    def plot_training_history(self, metrics):
        """Plot training and validation metrics"""
        plt.figure(figsize=(15, 10))

        # Plot loss
        plt.subplot(2, 2, 1)
        plt.plot(metrics['train_loss'], label='Train Loss')
        plt.plot(metrics['val_loss'], label='Validation Loss')
        plt.title('Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)

        # Plot accuracy
        plt.subplot(2, 2, 2)
        plt.plot(metrics['train_acc'], label='Train Accuracy')
        plt.plot(metrics['val_acc'], label='Validation Accuracy')
        plt.title('Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)

        # Plot top-3 and top-5 accuracy
        plt.subplot(2, 2, 3)
        plt.plot(metrics['val_acc'], label='Top-1 Accuracy')
        plt.plot(metrics['val_top3_acc'], label='Top-3 Accuracy')
        plt.plot(metrics['val_top5_acc'], label='Top-5 Accuracy')
        plt.title('Validation Accuracy Metrics')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)

        # Save the figure
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=300)
        plt.close()
        logger.info("Saved training history plot to training_history.png")


class ExplainableAI:
    """
    Provides model interpretability through various techniques
    """
    def __init__(self, model, preprocessor, device):
        self.model = model
        self.preprocessor = preprocessor
        self.device = device
        logger.info("Initialized ExplainableAI module")

    def generate_grad_cam(self, text, target_class=None):
        """Generate Grad-CAM visualization for character importance"""
        # Set model to eval mode
        self.model.eval()

        # Preprocess input
        cleaned_text = self.preprocessor._clean_text(text)
        encoded = self.preprocessor.char_encode_texts([cleaned_text])
        inputs = torch.tensor(encoded, dtype=torch.long, device=self.device)

        # Create hooks for the last convolutional layer
        last_conv_layer = None
        activation = None
        gradients = None

        def get_activation(module, input, output):
            nonlocal activation
            activation = output.detach()

        def get_gradients(module, grad_input, grad_output):
            nonlocal gradients
            gradients = grad_output[0].detach()

        # Find the last convolutional layer
        for name, module in reversed(list(self.model.named_modules())):
            if isinstance(module, nn.Conv1d):
                last_conv_layer = module
                break

        if last_conv_layer is None:
            logger.warning("Could not find convolutional layer for Grad-CAM")
            return None

        # Register hooks
        handle_fwd = last_conv_layer.register_forward_hook(get_activation)
        handle_bwd = last_conv_layer.register_full_backward_hook(get_gradients)

        # Forward pass
        self.model.zero_grad()
        output = self.model(inputs)

        # Determine target class
        if target_class is None:
            target_class = output.argmax(1).item()

        # Create one-hot encoding for target class
        one_hot = torch.zeros_like(output)
        one_hot[0, target_class] = 1

        # Backward pass
        output.backward(gradient=one_hot)

        # Remove hooks
        handle_fwd.remove()
        handle_bwd.remove()

        # Calculate weights
        weights = torch.mean(gradients, dim=(0, 2))

        # Apply weights to activations
        batch_size = activation.size(0)
        weighted_activations = torch.zeros(
            (batch_size, activation.size(2)),
            dtype=activation.dtype,
            device=activation.device
        )

        for i in range(weights.size(0)):
            weighted_activations += weights[i] * activation[:, i, :]

        # Apply ReLU to focus on features that have positive influence
        heatmap = F.relu(weighted_activations)

        # Normalize heatmap
        heatmap_max = heatmap.max(1, keepdim=True)[0]
        heatmap_min = heatmap.min(1, keepdim=True)[0]
        heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min + 1e-10)

        # Convert to numpy for visualization
        heatmap = heatmap.cpu().numpy()[0]

        # Visualize
        self._visualize_char_importance(cleaned_text, heatmap, target_class)

        return heatmap

    def _visualize_char_importance(self, text, heatmap, class_idx):
        """Visualize character importance as a heatmap"""
        # Interpolate heatmap to match text length
        from scipy.interpolate import interp1d

        # Make sure text is not longer than heatmap
        text = text[:len(heatmap)]

        # Make sure heatmap is not longer than text
        if len(heatmap) > len(text):
            heatmap = heatmap[:len(text)]

        # Interpolate if sizes don't match
        if len(heatmap) != len(text):
            x = np.linspace(0, 1, len(heatmap))
            f = interp1d(x, heatmap)
            x_new = np.linspace(0, 1, len(text))
            heatmap = f(x_new)

        # Plot
        plt.figure(figsize=(20, 4))
        plt.imshow([heatmap], cmap='YlOrRd', aspect='auto')
        plt.colorbar()
        plt.xticks(range(len(text)), list(text), rotation=45, fontsize=8)
        plt.yticks([])

        if self.preprocessor.label_encoder:
            class_name = self.preprocessor.label_encoder.inverse_transform([class_idx])[0]
            plt.title(f'Character Importance for Class: {class_name} (ID: {class_idx})')
        else:
            plt.title(f'Character Importance for Class ID: {class_idx}')

        plt.tight_layout()
        plt.savefig(f'char_importance_class_{class_idx}.png', dpi=150)
        plt.close()

        logger.info(f"Saved character importance visualization to char_importance_class_{class_idx}.png")

    def explain_prediction(self, text):
        """Provide comprehensive explanation for a prediction"""
        self.model.eval()

        # Preprocess text
        encoded = self.preprocessor.char_encode_texts([text])
        inputs = torch.tensor(encoded, dtype=torch.long, device=self.device)

        # Get prediction
        with torch.no_grad():
            outputs = self.model(inputs)
            probabilities = F.softmax(outputs, dim=1)

        # Get top predictions
        top_probs, top_indices = probabilities[0].topk(5)
        top_probs = top_probs.cpu().numpy()
        top_indices = top_indices.cpu().numpy()

        # Get predicted class
        pred_class = top_indices[0]
        pred_prob = top_probs[0]

        # Prepare explanation
        explanation = {
            'predicted_class_id': int(pred_class),
            'confidence': float(pred_prob),
            'top_5_predictions': []
        }

        # Add class names if available
        if self.preprocessor.label_encoder:
            class_names = self.preprocessor.label_encoder.inverse_transform(top_indices)
            explanation['predicted_class_name'] = class_names[0]
            for i in range(5):
                explanation['top_5_predictions'].append({
                    'class_id': int(top_indices[i]),
                    'class_name': class_names[i],
                    'probability': float(top_probs[i])
                })
        else:
            for i in range(5):
                explanation['top_5_predictions'].append({
                    'class_id': int(top_indices[i]),
                    'probability': float(top_probs[i])
                })

        # Generate attention heatmap
        heatmap = self.generate_grad_cam(text, pred_class)
        explanation['attention_heatmap'] = 'Generated and saved to file'

        return explanation


class AutomatedCrimeClassifier:
    """
    Main class that orchestrates the entire automated classification pipeline
    """
    def __init__(self, data_path, output_dir='output'):
        self.data_path = data_path
        self.output_dir = output_dir
        self.df = None
        self.preprocessor = None
        self.clusterer = None
        self.model = None
        self.trainer = None
        self.explainer = None
        self.device = get_device()

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

        logger.info(f"Initialized AutomatedCrimeClassifier with data from {data_path}")

    def load_data(self):
        """Load data from CSV file"""
        logger.info(f"Loading data from {self.data_path}")
        self.df = pd.read_csv(self.data_path)
        logger.info(f"Loaded {len(self.df)} rows of data")

        # Basic data info
        logger.info(f"Columns: {self.df.columns.tolist()}")
        logger.info(f"Sample data:\n{self.df.head()}")
        return self.df

    def preprocess_data(self):
        """Preprocess the text data"""
        if self.df is None:
            self.load_data()

        logger.info("Starting data preprocessing")
        self.preprocessor = DataPreprocessor(max_length=512, min_freq=3)

        # Build character vocabulary
        texts = self.df['crimeaditionalinfo'].values
        self.preprocessor.build_char_vocabulary(texts)

        # Create TF-IDF vectors for clustering
        tfidf_vectors = self.preprocessor.create_tfidf_vectors(texts)

        # Save preprocessor
        self.preprocessor.save(os.path.join(self.output_dir, 'preprocessor.pkl'))

        return tfidf_vectors

    def perform_clustering(self, vectors=None):
        """Perform automatic clustering to create pseudo-labels"""
        if vectors is None and self.df is None:
            self.load_data()
            vectors = self.preprocess_data()

        logger.info("Starting automatic clustering")
        self.clusterer = AutoClusterer(min_clusters=30, max_clusters=70)

        # Find optimal number of clusters
        optimal_clusters = self.clusterer.find_optimal_clusters(vectors)

        # Perform clustering
        cluster_labels = self.clusterer.perform_clustering(vectors)

        # Visualize clusters
        self.clusterer.visualize_clusters(
            vectors,
            output_file=os.path.join(self.output_dir, 'cluster_visualization.png')
        )

        logger.info(f"Created {optimal_clusters} clusters")

        # Add cluster labels to dataframe
        self.df['cluster_label'] = cluster_labels

        # Save clustered data
        self.df.to_csv(os.path.join(self.output_dir, 'clustered_data.csv'), index=False)

        return cluster_labels

    def prepare_training_data(self, cluster_labels=None):
        """Prepare data for model training"""
        logger.info("Preparing training data")

        # If cluster_labels not provided, get them from df
        if cluster_labels is None:
            if 'cluster_label' not in self.df.columns:
                raise ValueError("No cluster labels found. Run perform_clustering first.")
            cluster_labels = self.df['cluster_label'].values

        # Encode cluster labels
        encoded_labels = self.preprocessor.encode_labels(cluster_labels)

        # Character-encode text data
        texts = self.df['crimeaditionalinfo'].values
        X = self.preprocessor.char_encode_texts(texts)

        # Split data
        X_train, X_val, y_train, y_val = train_test_split(
            X, encoded_labels, test_size=0.2,
            random_state=42, stratify=encoded_labels
        )

        logger.info(f"Training data prepared: {X_train.shape[0]} train samples, {X_val.shape[0]} validation samples")

        # Create datasets and data loaders
        train_dataset = CrimeDataset(X_train, y_train)
        val_dataset = CrimeDataset(X_val, y_val)

        train_loader = DataLoader(
            train_dataset, batch_size=32, shuffle=True,
            num_workers=4, pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, batch_size=64, shuffle=False,
            num_workers=4, pin_memory=True
        )

        return train_loader, val_loader, self.preprocessor.vocab, len(np.unique(encoded_labels))

    def build_model(self, vocab_size, num_classes):
        """Build the classification model"""
        logger.info(f"Building model with vocab size {vocab_size} and {num_classes} classes")

        self.model = ComplexBilingualModel(
            vocab_size=vocab_size,
            num_classes=num_classes,
            max_length=self.preprocessor.max_length
        )

        self.trainer = ModelTrainer(self.model, self.device)

        # Log model summary
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        logger.info(f"Model created with {total_params:,} total parameters")
        logger.info(f"Trainable parameters: {trainable_params:,}")

        return self.model

    def train_model(self, train_loader, val_loader, epochs=30):
        """Train the model"""
        if self.trainer is None:
            raise ValueError("Model not built. Call build_model first.")

        logger.info(f"Starting model training for {epochs} epochs")
        metrics = self.trainer.train(train_loader, val_loader, epochs=epochs)

        # Save final model
        model_path = os.path.join(self.output_dir, 'final_model.pt')
        self.trainer.save_model(model_path)
        logger.info(f"Saved final model to {model_path}")

        return metrics

    def initialize_explainer(self):
        """Initialize the explainable AI module"""
        if self.model is None:
            raise ValueError("Model not built. Call build_model first.")

        logger.info("Initializing ExplainableAI module")
        self.explainer = ExplainableAI(self.model, self.preprocessor, self.device)
        return self.explainer

    def run_complete_pipeline(self, epochs=30):
        """Run the complete automated pipeline"""
        logger.info("Starting complete automated pipeline")

        # Step 1: Load data
        self.load_data()

        # Step 2: Preprocess data
        vectors = self.preprocess_data()

        # Step 3: Perform clustering
        self.perform_clustering(vectors)

        # Step 4: Prepare training data
        train_loader, val_loader, vocab_size, num_classes = self.prepare_training_data()

        # Step 5: Build model
        self.build_model(vocab_size, num_classes)

        # Step 6: Train model
        self.train_model(train_loader, val_loader, epochs=epochs)

        # Step 7: Initialize explainer
        self.initialize_explainer()

        logger.info("Complete pipeline execution finished successfully")

        return {
            'model': self.model,
            'trainer': self.trainer,
            'explainer': self.explainer,
            'preprocessor': self.preprocessor,
            'num_classes': num_classes
        }

    def predict(self, text):
        """Make prediction for a new text input"""
        if self.model is None:
            raise ValueError("Model not trained. Run pipeline first.")

        # Preprocess text
        encoded = self.preprocessor.char_encode_texts([text])
        inputs = torch.tensor(encoded, dtype=torch.long, device=self.device)

        # Make prediction
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(inputs)
            probabilities = F.softmax(outputs, dim=1)

        # Get top prediction
        top_prob, top_idx = probabilities[0].max(0)

        result = {
            'predicted_class_id': int(top_idx.item()),
            'confidence': float(top_prob.item())
        }

        # Add class name if available
        if self.preprocessor.label_encoder:
            result['predicted_class_name'] = self.preprocessor.label_encoder.inverse_transform([top_idx.item()])[0]

        return result

    def explain_prediction(self, text):
        """Explain a prediction"""
        if self.explainer is None:
            self.initialize_explainer()

        return self.explainer.explain_prediction(text)


# Main execution
if __name__ == "__main__":
    # Configuration
    DATA_FILE = "/home/i4cuser8/prj/db/crime_report.csv"  # Update with your file path
    OUTPUT_DIR = "crime_classifier_output"
    EPOCHS = 100

    # Run automated pipeline
    classifier = AutomatedCrimeClassifier(DATA_FILE, OUTPUT_DIR)
    results = classifier.run_complete_pipeline(epochs=EPOCHS)

    # Example: Making predictions
    example_text = "      "  # Sample Hindi text
    prediction = classifier.predict(example_text)
    print(f"Prediction: {prediction}")

    # Example: Explaining prediction
    explanation = classifier.explain_prediction(example_text)
    print(f"Explanation: {json.dumps(explanation, indent=2)}")